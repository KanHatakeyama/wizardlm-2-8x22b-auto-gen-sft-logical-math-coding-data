{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# %%\n",
    "# %%\n",
    "import os\n",
    "from datetime import datetime\n",
    "from vllm import SamplingParams, LLM\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "args = sys.argv\n",
    "import time\n",
    "from categories import *\n",
    "\n",
    "\n",
    "def get_longest_phrase_length(text):\n",
    "    # 区切り文字として、スペース、カンマ、句読点、改行を指定\n",
    "    delimiters = r'[ ,。！？、\\n]'\n",
    "    # テキストを区切り文字で分割\n",
    "    try:\n",
    "        phrases = re.split(delimiters, text)\n",
    "        # 最大のフレーズの長さを取得\n",
    "        max_length = max(len(phrase) for phrase in phrases)\n",
    "    except:\n",
    "        max_length=9999\n",
    "    return max_length\n",
    "\n",
    "def is_abnormal_text(text, threshold=40):\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    # 複数の区切り文字をカウント\n",
    "    period_count = text.count('.') + text.count(',') + text.count('､') + text.count('｡')\n",
    "    ratio = word_count / period_count if period_count > 0 else word_count\n",
    "    return ratio > threshold\n",
    "\n",
    "def is_good_sentence(sentence):\n",
    "    if get_longest_phrase_length(sentence)>100:\n",
    "        return False\n",
    "    if is_abnormal_text(sentence):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "batch_size=10\n",
    "max_count=10**5\n",
    "out_dir = \"0805wizard8x22b\"\n",
    "\n",
    "pid = os.getpid()\n",
    "seed=int(pid)+int(datetime.now().timestamp())\n",
    "print(\"seed: \",seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# %%\n",
    "os.system(f\"mkdir -p {out_dir}\")\n",
    "\n",
    "current_time_no_symbols = datetime.now().strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\").replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"\")\n",
    "rand_id=random.randint(0,10000)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "model_name = \"alpindale/WizardLM-2-8x22B\"\n",
    "tensor_parallel_size=4\n",
    "llm = LLM(model=model_name, trust_remote_code=True,\n",
    "          max_model_len=4000,\n",
    "          # max_model_len=7000,\n",
    "         #  gpu_memory_utilization=0.9,\n",
    "         tensor_parallel_size=tensor_parallel_size,\n",
    "          )\n",
    "\n",
    "def llm_gen(llm,prompt_list,temperature=0.7,top_k=50):\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        prompt_list,\n",
    "        sampling_params=SamplingParams(\n",
    "            temperature=temperature,\n",
    "            max_tokens=2048,\n",
    "            repetition_penalty=1.1,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "    )\n",
    "    return [i.outputs[0].text.strip() for i in outputs]\n",
    "\n",
    "\n",
    "# %%\n",
    "def question_to_prompt(question,role=\"an artificial intelligence assistant\"):\n",
    "    prompt=f\"\"\"A chat between a curious user and {role}. The assistant gives helpful, \n",
    "detailed, and polite answers to the user's questions. USER: {question} ASSISTANT:\"\"\"\n",
    "    return prompt\n",
    "def question_to_prompt_2nd_turn(q1,a1,q2,role=\"an artificial intelligence assistant\"):\n",
    "    prompt=f\"\"\"A chat between a curious user and {role}. The assistant gives helpful, \n",
    "detailed, and polite answers to the user's questions. USER: {q1} ASSISTANT: {a1} USER: {q2} ASSISTANT:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_question_commands=[\n",
    "\"For the following Q&A, generate an additional short question that change the assumptions.\",\n",
    "\"For the following Q&A, generate an additional short question.\",\n",
    "\"For the following Q&A, generate an additional short question that is related to the previous question.\",\n",
    "\"For the following Q&A, generate an additional short question that is related to the previous question but is more general.\",\n",
    "\"For the following Q&A, generate an additional short question that is related to the previous question but is more specific.\",\n",
    "\"For the following Q&A, generate an additional short question that is related to the previous question that doubts the assumptions.\",\n",
    "\"For the following Q&A, generate an additional short question that is related to the previous question that doubts the conclusions.\",\n",
    "\"For the following Q&A, generate an additional short question that is related to the previous question that is more complex.\",\n",
    "\"For the following Q&A, generate an additional short question that introduces a new perspective.\",\n",
    "\"For the following Q&A, generate an additional short question that challenges the premises.\",\n",
    "\"For the following Q&A, generate an additional short question that supports the answer with further details.\",\n",
    "\"For the following Q&A, generate an additional short question that requires a different type of reasoning.\",\n",
    "\"For the following Q&A, generate an additional short question that explores potential exceptions.\",\n",
    "\"For the following Q&A, generate an additional short question that examines the implications.\",\n",
    "\"For the following Q&A, generate an additional short question that suggests an alternative solution.\",\n",
    "\"For the following Q&A, generate an additional short question that tests the robustness of the answer.\",\n",
    "\"For the following Q&A, generate an additional short question that considers a counterexample.\",\n",
    "\"For the following Q&A, generate an additional short question that connects the topic to a broader context.\",\n",
    "\"For the following Q&A, generate an additional short question that requires applying the concept to a practical scenario.\",\n",
    "\"For the following Q&A, generate an additional short question that probes deeper into the underlying principles.\",\n",
    "\"For the following Q&A, generate an additional short question that explores a related subtopic.\",\n",
    "\"For the following Q&A, generate an additional short question that contrasts with the initial question.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count=0\n",
    "file_id=0\n",
    "while True:\n",
    "    seed=int(pid)+int(datetime.now().timestamp())\n",
    "    print(\"seed: \",seed)\n",
    "    random.seed(seed)\n",
    "    prompt_list=[]\n",
    "\n",
    "\n",
    "    #q1\n",
    "    for qid in range(batch_size):\n",
    "        job=random.choice(job_list)\n",
    "        character=random.choice(character_list)\n",
    "        role=f\"{job}. You are {character}\"\n",
    "        genre=random.choice(genre_list)+\",\"+random.choice(genre_list)\n",
    "        level=random.choice(levels)\n",
    "        command=f\"\"\"Prepare a mathematical, reasoning, logical, or coding problem or quiz.\n",
    "- Output only the question, which is not too long.\n",
    "- NEVER the answer or any other things.\n",
    "- Topic: {genre}.\n",
    "- Level: {level}.\n",
    "\"\"\"\n",
    "        prompt_list.append(question_to_prompt(command,role))\n",
    "\n",
    "    print(prompt_list[:3])  \n",
    "    first_question_list=llm_gen(llm,prompt_list,temperature=0.01)\n",
    "\n",
    "    #a1\n",
    "    prompt_list=[question_to_prompt(q) for q in first_question_list]\n",
    "    first_answer_list=llm_gen(llm,prompt_list,temperature=0.01)\n",
    "\n",
    "    #q2\n",
    "    prompt_list=[]\n",
    "    for q1,a1 in zip(first_question_list,first_answer_list):\n",
    "        command=random.choice(additional_question_commands)\n",
    "        question=f\"\"\"{command}\n",
    "- Never output answer, only output question.\n",
    "[Q&A start]\n",
    "Question: {q1}\n",
    "Answer: {a1}\n",
    "[Q&A end]\"\"\"\n",
    "        prompt_list.append(question_to_prompt(question))\n",
    "\n",
    "    second_question_list=llm_gen(llm,prompt_list,temperature=0.7)\n",
    "\n",
    "    #a2\n",
    "    prompt_list=[question_to_prompt_2nd_turn(q1,a1,q2) for q1,a1,q2 in zip(first_question_list,first_answer_list,second_question_list)]\n",
    "    second_answer_list=llm_gen(llm,prompt_list,temperature=0.01)\n",
    "\n",
    "\n",
    "\n",
    "    out_path = f\"{out_dir}/model_{current_time_no_symbols}_{rand_id}_{file_id}.jsonl\"\n",
    "\n",
    "    #書き出し    \n",
    "\n",
    "    with open(out_path, \"a\") as f:\n",
    "        for q1,a1,q2,a2 in zip(first_question_list,first_answer_list,second_question_list,second_answer_list):\n",
    "            covnersation_list=[]\n",
    "\n",
    "            #1st turn\n",
    "            if not is_good_sentence(q1):\n",
    "                continue\n",
    "            if not is_good_sentence(a1):\n",
    "                continue\n",
    "\n",
    "            covnersation_list.append({\"role\":\"user\",\"content\":q1})\n",
    "            covnersation_list.append({\"role\":\"assistant\",\"content\":a1})\n",
    "\n",
    "            #2nd turn\n",
    "            if is_good_sentence(q2) and is_good_sentence(a2) and len(q2)>10 and len(a2)>10:\n",
    "                covnersation_list.append({\"role\":\"user\",\"content\":q2})\n",
    "                covnersation_list.append({\"role\":\"assistant\",\"content\":a2})\n",
    "\n",
    "            record={}\n",
    "            record[\"messages\"]=covnersation_list\n",
    "\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")                                                                                   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
